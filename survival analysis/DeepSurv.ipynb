{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepSurv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkcEWbrHXvNr",
        "outputId": "94da68da-2941-4d71-c87e-f3a0e083725b"
      },
      "source": [
        "!pip install lifelines"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lifelines\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ba/d010b22c8bcdfe3bbba753bd976f5deddfa4ec1c842b991579e9c2c3cd61/lifelines-0.26.0-py3-none-any.whl (348kB)\n",
            "\r\u001b[K     |█                               | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 18.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 16.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 14.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 92kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 317kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 327kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 337kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 7.7MB/s \n",
            "\u001b[?25hCollecting autograd-gamma>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/85/ae/7f2031ea76140444b2453fa139041e5afd4a09fc5300cfefeb1103291f80/autograd-gamma-0.5.0.tar.gz\n",
            "Collecting formulaic<0.3,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/64/6702b5cadc89ece93af2e01996504f3a895196354a35713e2ef22f089d3e/formulaic-0.2.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n",
            "Collecting interface-meta>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/71/31/5e474208f5df9012ebecfaa23884b14f93671ea4f4f6d468eb096b73e499/interface_meta-1.2.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.12.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->lifelines) (1.15.0)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-cp37-none-any.whl size=4050 sha256=a7ffbfb20a9dd5c98f1fbd8dc3c9759ee25667b134a71012cfa01ea5f630984a\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/68/dc/91321c55fba449755524481854f5be70d41912b8f886f908bb\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: autograd-gamma, interface-meta, formulaic, lifelines\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-0.2.3 interface-meta-1.2.3 lifelines-0.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhCQroRM_Gbo"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as Data\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from lifelines.utils import concordance_index\n",
        "import csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU9_v5g9_bxI"
      },
      "source": [
        "class Regularization(object):\n",
        "    def __init__(self, order, weight_decay):\n",
        "        super(Regularization, self).__init__()\n",
        "        self.order = order\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def __call__(self, model):\n",
        "        reg_loss = 0\n",
        "        for name, w in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                reg_loss = reg_loss + torch.norm(w, p=self.order)\n",
        "        reg_loss = self.weight_decay * reg_loss\n",
        "        return reg_loss"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-roEWy-FAA3g"
      },
      "source": [
        "class DeepSurv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepSurv, self).__init__()\n",
        "        # parses parameters of network from configuration\n",
        "        self.hidden_1 = nn.Linear(1046, 10)\n",
        "        self.bat_1 = nn.BatchNorm1d(10)\n",
        "        self.do_1 = nn.Dropout(0.5)\n",
        "        self.hidden_2 = nn.Linear(10,5)\n",
        "        self.bat_2 = nn.BatchNorm1d(5)\n",
        "        self.do_2 = nn.Dropout(0.5)\n",
        "        self.output = nn.Linear(5,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.do_1(nn.functional.selu(self.bat_1(self.hidden_1(x))))\n",
        "        x = self.do_2(nn.functional.selu(self.bat_2(self.hidden_2(x))))\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh6KqBqs_4os"
      },
      "source": [
        "class NegativeLogLikelihood(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NegativeLogLikelihood, self).__init__()\n",
        "        self.L2_reg = 0\n",
        "        self.reg = Regularization(order=2, weight_decay=self.L2_reg)\n",
        "\n",
        "    def forward(self, risk_pred, y, e, model):\n",
        "        idx = y.sort(descending=True)[1]\n",
        "        events = e[idx]\n",
        "        risk_pred = risk_pred[idx]\n",
        "        events = events.float()\n",
        "        events = events.view(-1)\n",
        "        risk_pred = risk_pred.view(-1)\n",
        "        uncensored_likelihood = risk_pred - risk_pred.exp().cumsum(0).log()\n",
        "        censored_likelihood = uncensored_likelihood * events\n",
        "        num_observed_events = events.sum()\n",
        "        neg_likelihood = -censored_likelihood.sum()/num_observed_events\n",
        "        l2_loss = self.reg(model)\n",
        "        return neg_likelihood + l2_loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPGSIbAVORXm"
      },
      "source": [
        "def c_index(risk_pred, y, e):\n",
        "    if not isinstance(y, np.ndarray):\n",
        "        y = y.detach().cpu().numpy()\n",
        "    if not isinstance(risk_pred, np.ndarray):\n",
        "        risk_pred = risk_pred.detach().cpu().numpy()\n",
        "    if not isinstance(e, np.ndarray):\n",
        "        e = e.detach().cpu().numpy()\n",
        "    return concordance_index(y, risk_pred, e)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1b9Uji0Qpzk"
      },
      "source": [
        "class Dataloader(Dataset):\n",
        "  def __init__(self,filename,is_train):\n",
        "      self.x,self.y,self.e = self.dataset_construction(filename,is_train)\n",
        "  \n",
        "  def dataset_construction(self, filename, is_train):\n",
        "      with open(filename,'rt') as raw_data:\n",
        "        data=np.loadtxt(raw_data,delimiter=',')\n",
        "\n",
        "      if is_train:\n",
        "        x = data[:60,3:].astype(np.float32)\n",
        "        y = data[:60,0].astype(np.float32).reshape(-1,1)\n",
        "        e = data[:60,1].reshape(-1,1)\n",
        "\n",
        "      if not is_train:\n",
        "        x = data[60:,3:].astype(np.float32)\n",
        "        y = data[60:,0].astype(np.float32).reshape(-1,1)\n",
        "        e = data[60:,1].reshape(-1,1)\n",
        "\n",
        "      return x,y,e\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "      \n",
        "      x_item = self.x[item]\n",
        "      e_item = self.e[item] \n",
        "      y_item = self.y[item] \n",
        "      \n",
        "      x_tensor = torch.from_numpy(x_item)\n",
        "      e_tensor = torch.from_numpy(e_item)\n",
        "      y_tensor = torch.from_numpy(y_item)\n",
        "      return x_tensor, y_tensor, e_tensor\n",
        "  \n",
        "  def __len__(self):\n",
        "      return self.x.shape[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDinhxATPKWu"
      },
      "source": [
        "def train():\n",
        "    model = DeepSurv()\n",
        "    criterion = NegativeLogLikelihood()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    train_dataset = Dataloader(filename=\"ACC.csv\",is_train=True)\n",
        "    test_dataset = Dataloader(filename=\"ACC.csv\",is_train=False)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=20)\n",
        "    best_c_index = 0\n",
        "    for epoch in range(0,500):\n",
        "      model.train()\n",
        "      for x,y,e in train_loader:\n",
        "        risk_pred = model(x)\n",
        "        #print(risk_pred)\n",
        "        train_loss = criterion(risk_pred,y,e,model)\n",
        "        #print(train_loss)\n",
        "        train_c = c_index(-risk_pred,y,e)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      model.eval()\n",
        "      for x,y,e in test_loader:\n",
        "        with torch.no_grad():\n",
        "          risk_pred = model(x)\n",
        "          valid_loss = criterion(risk_pred,y,e,model)\n",
        "          valid_c = c_index(-risk_pred,y,e)\n",
        "          #print(valid_c)\n",
        "          if best_c_index < valid_c:\n",
        "            best_c_index = valid_c\n",
        "        \n",
        "    return best_c_index"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCmKSP_TX5Ah",
        "outputId": "ddd43999-09ad-4c82-82bc-649155120e04"
      },
      "source": [
        "result=train()\n",
        "result"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45901639344262296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWYc13dgX9Sf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}